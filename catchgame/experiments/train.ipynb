{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    os.chdir(os.path.join(os.getcwd(), '../../catchgame/'))\n",
    "except:\n",
    "    print(\"already in directory\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device} is available\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb \n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "experiment_log = []\n",
    "\n",
    "# function version\n",
    "def save_experiment(outfolder=\"./runs\"):\n",
    "    run_name = \"run-\" + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    os.makedirs(outfolder, exist_ok=True)\n",
    "    os.makedirs(os.path.join(outfolder, run_name), exist_ok=True)\n",
    "    keys = experiment_log[0].keys()\n",
    "    for key in keys:\n",
    "        np.save(os.path.join(outfolder, run_name, f\"{key}.npy\"), np.array([log.get(key, None) for log in experiment_log]))\n",
    "\n",
    "    df = pd.DataFrame(experiment_log)\n",
    "    df.to_csv(os.path.join(outfolder, run_name, \"log.csv\"), index=False)\n",
    "\n",
    "    print(f\"Experiment saved to {os.path.join(outfolder, run_name)}\")\n",
    "\n",
    "def init_experiment(config):\n",
    "    wandb.init(\n",
    "        project=\"rug-drl-catchgame\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "def terminate_experiment(save=True):\n",
    "    wandb.finish()\n",
    "    if save:\n",
    "        save_experiment()\n",
    "\n",
    "def log(data):\n",
    "    experiment_log.append(data)\n",
    "    print(\"; \".join([f\"{k}: {v}\" for k, v in data.items()]))\n",
    "    wandb.log(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dqn.network import QNetworkConv\n",
    "from world import CatchEnv\n",
    "\n",
    "n_actions = 3\n",
    "n_episodes = 1000\n",
    "batch_size = 512\n",
    "\n",
    "CATCH_ENV_NUM_STATES = 4\n",
    "CATCH_ENV_WORLD_SIZE = (84, 84)\n",
    "CATCH_ENV_POSSIBLE_ACTIONS = [0, 1, 2]\n",
    "\n",
    "spatial_size = CATCH_ENV_WORLD_SIZE\n",
    "\n",
    "dqn = QNetworkConv(\n",
    "    in_channels=CATCH_ENV_NUM_STATES, \n",
    "    spatial_size=spatial_size, \n",
    "    output_size=len(CATCH_ENV_POSSIBLE_ACTIONS)\n",
    ").to(device)\n",
    "dqn_target = QNetworkConv(\n",
    "    in_channels=CATCH_ENV_NUM_STATES,\n",
    "    spatial_size=spatial_size,\n",
    "    output_size=len(CATCH_ENV_POSSIBLE_ACTIONS)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84, 84, 4), 0, False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = CatchEnv()\n",
    "env.reset_random()\n",
    "next_state, reward, done = env.step(action=1)\n",
    "next_state.shape, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mo\\anaconda3\\envs\\ml_env\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 84, 84])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize(spatial_size),\n",
    "    lambda x: x.unsqueeze(0),\n",
    "    lambda x: x.type(torch.float32),\n",
    "])\n",
    "\n",
    "transforms(next_state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(dqn.parameters(), lr=1e-4)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "num_episodes = 1000\n",
    "epsilon = 0.8\n",
    "gamma = 0.9\n",
    "epsilon_decay = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define a replay buffer class to store experiences\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, done_flags = zip(*batch)\n",
    "        return states, actions, rewards, next_states, done_flags\n",
    "\n",
    "# Initialize the replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "def evaulate(epsilon=0.0, num_episodes=10):\n",
    "    rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset_random()\n",
    "        state = transforms(state)\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = random.choice(CATCH_ENV_POSSIBLE_ACTIONS)\n",
    "            else:\n",
    "                q_values = dqn(state.to(device))\n",
    "                action = q_values.argmax().item()\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = transforms(next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "        rewards.append(episode_reward)\n",
    "    return np.mean(rewards)\n",
    "\n",
    "def sample_epsilon_exponential_decay(min_epsilon=0.01, max_epsilon=0.9, t=0.0, decay_rate=0.5):\n",
    "    I = max_epsilon\n",
    "    E = min_epsilon\n",
    "    s = 10 * decay_rate\n",
    "    theta = (np.exp(-s*t) -1) / (1 - np.exp(-s)) + 1\n",
    "    return (I - E) * theta + E"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmassaf\u001b[0m (\u001b[33mrug-ai-group\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\mo\\Documents\\Organisations\\RUG\\rug-drl\\catchgame\\wandb\\run-20230515_100323-51c04ljh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rug-ai-group/rug-drl-catchgame/runs/51c04ljh' target=\"_blank\">bright-donkey-2</a></strong> to <a href='https://wandb.ai/rug-ai-group/rug-drl-catchgame' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rug-ai-group/rug-drl-catchgame' target=\"_blank\">https://wandb.ai/rug-ai-group/rug-drl-catchgame</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rug-ai-group/rug-drl-catchgame/runs/51c04ljh' target=\"_blank\">https://wandb.ai/rug-ai-group/rug-drl-catchgame/runs/51c04ljh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0; epsilon: 0.0; reward: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:00<02:51,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0; mean_reward: 0.3; epsilon: 0.3817216991062283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/1000 [00:00<00:28, 35.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10; mean_reward: 0.2; epsilon: 0.5299195944432384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 30/1000 [00:00<00:22, 44.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 20; mean_reward: 0.1; epsilon: 0.7299658287281285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 36/1000 [00:01<00:26, 36.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 30; mean_reward: 0.2; epsilon: 0.5299195944432384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/1000 [00:01<00:29, 33.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 40; mean_reward: 0.2; epsilon: 0.5299195944432384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 51/1000 [00:03<02:27,  6.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 50; mean_reward: 0.3; epsilon: 0.3817216991062283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 61/1000 [00:08<06:13,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 60; mean_reward: 0.3; epsilon: 0.3817216991062283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 71/1000 [00:13<08:24,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 70; mean_reward: 0.4; epsilon: 0.27193399797388856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 81/1000 [00:19<08:20,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 80; mean_reward: 0.3; epsilon: 0.3817216991062283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 91/1000 [00:24<08:16,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 90; mean_reward: 0.2; epsilon: 0.5299195944432384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 101/1000 [00:29<07:53,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 100; mean_reward: 0.4; epsilon: 0.27193399797388856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 111/1000 [00:34<08:20,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 110; mean_reward: 0.5; epsilon: 0.19060126856829268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 113/1000 [00:35<08:09,  1.81it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "init_experiment(config={\n",
    "    \"num_episodes\": num_episodes,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"gamma\": gamma,\n",
    "    \"epsilon_decay\": epsilon_decay,\n",
    "    \"spatial_size\": spatial_size,\n",
    "})\n",
    "\n",
    "################ TRAINING ################\n",
    "\n",
    "best_performance = evaulate(epsilon=0.0, num_episodes=10)\n",
    "log({\"episode\": 0, \"epsilon\": 0.0, \"reward\": best_performance})\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset_random()\n",
    "    state = transforms(state)\n",
    "    done = False\n",
    "    \n",
    "    done_i = 0\n",
    "\n",
    "    while not done:\n",
    "        # Choose an action using an epsilon-greedy policy\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(CATCH_ENV_POSSIBLE_ACTIONS)\n",
    "        else:\n",
    "            q_values = dqn(state.to(device))\n",
    "            action = q_values.argmax().item()\n",
    "        \n",
    "        # Take the action and observe the next state and reward\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state = transforms(next_state)\n",
    "        \n",
    "        # Add the experience to the replay buffer\n",
    "        replay_buffer.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Sample a batch of experiences from the replay buffer\n",
    "        if len(replay_buffer.buffer) > batch_size:\n",
    "            states, actions, rewards, next_states, done_flags = replay_buffer.sample(batch_size)\n",
    "            # (batch, 4, 84, 84)\n",
    "            # [(1, 4, 84, 84), ... ] -> (batch, 4, 84, 84)\n",
    "            \n",
    "            states = torch.cat(states).to(device)\n",
    "            actions = torch.Tensor(actions).type(dtype=torch.int64).to(device)\n",
    "            rewards = torch.Tensor(rewards).to(device)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            done_flags = torch.Tensor(done_flags).to(device)\n",
    "\n",
    "            # Compute the target Q-values using the DDQN algorithm\n",
    "            q_values = dqn(states)\n",
    "            max_actions = q_values.argmax(dim=-1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values_target = dqn_target(next_states)\n",
    "            q_values_target = q_values_target.gather(1, max_actions.unsqueeze(-1)).squeeze(-1)\n",
    "            q_values_target = rewards + gamma * q_values_target * (1 - done_flags)\n",
    "            \n",
    "            # Compute the current Q-values and the loss\n",
    "            current_q_values = q_values.gather(1, torch.Tensor(actions).unsqueeze(-1)).squeeze(-1)\n",
    "            loss = criterion(current_q_values, q_values_target.detach())\n",
    "            \n",
    "            # Update the DQN network\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the target network\n",
    "            if done_i % 10 == 0:\n",
    "                dqn_target.load_state_dict(dqn.state_dict())\n",
    "            done_i += 1\n",
    "        \n",
    "        # Update the state\n",
    "        state = next_state\n",
    "\n",
    "    # Evaluate the policy and save the results\n",
    "    if episode % 10 == 0:\n",
    "        mean_reward = evaulate(epsilon=0.0, num_episodes=10)\n",
    "        epsilon = sample_epsilon_exponential_decay(t=mean_reward, decay_rate=0.3, max_epsilon=1)\n",
    "        if mean_reward > best_performance:\n",
    "            best_performance = mean_reward\n",
    "            torch.save(dqn.state_dict(), \"models/dqn_best.pt\")\n",
    "        log({\"episode\": episode, \"mean_reward\": mean_reward, \"epsilon\": epsilon})\n",
    "\n",
    "################ TRAINING END ################\n",
    "\n",
    "terminate_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best \n",
    "dqn.load_state_dict(torch.load(\"models/dqn_best.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mo\\anaconda3\\envs\\ml_env\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evluation\n",
    "env = CatchEnv()\n",
    "state = env.reset_random()\n",
    "state = transforms(state)\n",
    "video = []\n",
    "\n",
    "while True:\n",
    "    q_values = dqn(state.to(device))\n",
    "    action = q_values.argmax().item()\n",
    "    next_state, reward, done = env.step(action)\n",
    "    next_state = transforms(next_state)\n",
    "    state = next_state\n",
    "    video.append(next_state.squeeze(0))\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "frames = []\n",
    "video = torch.stack(video)\n",
    "# shape (k iterations, 4 frames, 84, 84)\n",
    "# put side to side resulting in (4 * k, 84, 84) without changing the order\n",
    "video = video.permute(1, 0, 2, 3).reshape(4 * len(video), 84, 84)\n",
    "\n",
    "# video is of shape (time, 84, 84)\n",
    "# make a gif and display it\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "\n",
    "imageio.mimsave(\"dqn_conv.gif\", 255 * video.cpu().numpy())\n",
    "Image(filename=\"dqn_conv.gif\", format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
