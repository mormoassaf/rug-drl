{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    os.chdir(os.path.join(os.getcwd(), '../../catchgame/'))\n",
    "except:\n",
    "    print(\"already in directory\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device} is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.dqn.network import QNetworkConv\n",
    "from world import CatchEnv\n",
    "\n",
    "n_actions = 3\n",
    "n_episodes = 1000\n",
    "batch_size = 32\n",
    "\n",
    "CATCH_ENV_NUM_STATES = 4\n",
    "CATCH_ENV_WORLD_SIZE = (84, 84)\n",
    "CATCH_ENV_POSSIBLE_ACTIONS = [0, 1, 2]\n",
    "\n",
    "spatial_size = CATCH_ENV_WORLD_SIZE\n",
    "\n",
    "dqn = QNetworkConv(\n",
    "    in_channels=CATCH_ENV_NUM_STATES, \n",
    "    spatial_size=spatial_size, \n",
    "    output_size=len(CATCH_ENV_POSSIBLE_ACTIONS)\n",
    ").to(device)\n",
    "dqn_target = QNetworkConv(\n",
    "    in_channels=CATCH_ENV_NUM_STATES,\n",
    "    spatial_size=spatial_size,\n",
    "    output_size=len(CATCH_ENV_POSSIBLE_ACTIONS)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84, 84, 4), 0, False)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = CatchEnv()\n",
    "env.reset_random()\n",
    "next_state, reward, done = env.step(action=1)\n",
    "next_state.shape, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 84, 84])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize(spatial_size),\n",
    "    lambda x: x.unsqueeze(0),\n",
    "    lambda x: x.type(torch.float32),\n",
    "])\n",
    "\n",
    "transforms(next_state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(dqn.parameters(), lr=1e-4)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "\n",
    "num_episodes = 1000\n",
    "epsilon = 0.8\n",
    "gamma = 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mo\\anaconda3\\envs\\ml_env\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(actions)\u001b[39m.\u001b[39mtype(dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint64)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     61\u001b[0m rewards \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(rewards)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 62\u001b[0m next_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(next_states)\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     63\u001b[0m done_flags \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(done_flags)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     65\u001b[0m \u001b[39m# Compute the target Q-values using the DDQN algorithm\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define a replay buffer class to store experiences\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        done_flags = []\n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done_flag = experience\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            done_flags.append(done_flag)\n",
    "        return states, actions, rewards, next_states, done_flags\n",
    "\n",
    "# Initialize the replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "# Define the main training loop\n",
    "for episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset_random()\n",
    "    state = transforms(state)\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Choose an action using an epsilon-greedy policy\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = random.choice(CATCH_ENV_POSSIBLE_ACTIONS)\n",
    "        else:\n",
    "            q_values = dqn(state.to(device))\n",
    "            action = q_values.argmax().item()\n",
    "        \n",
    "        # Take the action and observe the next state and reward\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state = transforms(next_state)\n",
    "        \n",
    "        # Add the experience to the replay buffer\n",
    "        replay_buffer.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Sample a batch of experiences from the replay buffer\n",
    "        if len(replay_buffer.buffer) > batch_size:\n",
    "            states, actions, rewards, next_states, done_flags = replay_buffer.sample(batch_size)\n",
    "            # (batch, 4, 84, 84)\n",
    "            # [(1, 4, 84, 84), ... ] -> (batch, 4, 84, 84)\n",
    "            \n",
    "            states = torch.cat(states).to(device)\n",
    "            actions = torch.Tensor(actions).type(dtype=torch.int64).to(device)\n",
    "            rewards = torch.Tensor(rewards).to(device)\n",
    "            next_states = torch.cat(next_states).to(device)\n",
    "            done_flags = torch.Tensor(done_flags).to(device)\n",
    "\n",
    "            # Compute the target Q-values using the DDQN algorithm\n",
    "            q_values = dqn(states)\n",
    "            max_actions = q_values.argmax(dim=-1)\n",
    "\n",
    "            q_values_target = dqn_target(next_states)\n",
    "            q_values_target = q_values_target.gather(1, max_actions.unsqueeze(-1)).squeeze(-1)\n",
    "            q_values_target = rewards + gamma * q_values_target * (1 - done_flags)\n",
    "            \n",
    "            # Compute the current Q-values and the loss\n",
    "            current_q_values = q_values.gather(1, torch.Tensor(actions).unsqueeze(-1)).squeeze(-1)\n",
    "            loss = criterion(current_q_values, q_values_target.detach())\n",
    "            \n",
    "            # Update the DQN network\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the target network\n",
    "            dqn_target.load_state_dict(dqn.state_dict())\n",
    "        \n",
    "        # Update the state\n",
    "        state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
